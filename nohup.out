[2024-02-11 17:56:27,936] torch.distributed.run: [WARNING] 
[2024-02-11 17:56:27,936] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 17:56:27,936] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 17:56:27,936] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 17:56:34,412] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 17:56:34,412] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num_devices: 2
max_steps: 625
num_devices: 2
max_steps: 625
[2024-02-11 17:56:37,944] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 17:56:37,945] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-02-11 17:56:37,945] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[2024-02-11 17:56:46,075] [INFO] [partition_parameters.py:349:__exit__] finished initializing model - num_params = 341, num_elems = 1.42B
[2024-02-11 17:56:51,485][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-11 17:56:51,862] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
Parameter Offload: Total persistent parameters: 544768 in 194 params
  0%|          | 0/625 [00:00<?, ?it/s]/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/625 [00:07<1:21:19,  7.82s/it]  0%|          | 2/625 [00:10<49:16,  4.75s/it]    0%|          | 3/625 [00:12<38:12,  3.69s/it]  1%|          | 4/625 [00:15<33:00,  3.19s/it]  1%|          | 5/625 [00:17<30:16,  2.93s/it]  1%|          | 6/625 [00:20<28:33,  2.77s/it]  1%|          | 7/625 [00:22<27:00,  2.62s/it]  1%|▏         | 8/625 [00:24<26:25,  2.57s/it]  1%|▏         | 9/625 [00:27<25:43,  2.51s/it]  2%|▏         | 10/625 [00:29<25:21,  2.47s/it]  2%|▏         | 11/625 [00:32<26:11,  2.56s/it]  2%|▏         | 12/625 [00:34<25:20,  2.48s/it]  2%|▏         | 13/625 [00:37<24:43,  2.42s/it]  2%|▏         | 14/625 [00:39<24:14,  2.38s/it]  2%|▏         | 15/625 [00:41<24:19,  2.39s/it]  3%|▎         | 16/625 [00:44<23:58,  2.36s/it]  3%|▎         | 17/625 [00:46<24:17,  2.40s/it]  3%|▎         | 18/625 [00:48<23:55,  2.36s/it]  3%|▎         | 19/625 [00:51<23:39,  2.34s/it]  3%|▎         | 20/625 [00:53<23:29,  2.33s/it]  3%|▎         | 21/625 [00:55<23:21,  2.32s/it]  4%|▎         | 22/625 [00:58<23:14,  2.31s/it]  4%|▎         | 23/625 [01:00<23:40,  2.36s/it]  4%|▍         | 24/625 [01:02<23:28,  2.34s/it]  4%|▍         | 25/625 [01:05<23:20,  2.33s/it]  4%|▍         | 26/625 [01:07<23:17,  2.33s/it]  4%|▍         | 27/625 [01:09<23:09,  2.32s/it]  4%|▍         | 28/625 [01:12<23:22,  2.35s/it]  5%|▍         | 29/625 [01:14<23:13,  2.34s/it]  5%|▍         | 30/625 [01:16<23:05,  2.33s/it]  5%|▍         | 31/625 [01:19<23:56,  2.42s/it]                                                {'loss': 1.9199, 'learning_rate': 1e-05, 'epoch': 0.25}
  5%|▍         | 31/625 [01:19<23:56,  2.42s/it]  5%|▌         | 32/625 [01:22<24:27,  2.48s/it]  5%|▌         | 33/625 [01:24<25:38,  2.60s/it]  5%|▌         | 34/625 [01:27<24:40,  2.51s/it]  6%|▌         | 35/625 [01:29<23:57,  2.44s/it]  6%|▌         | 36/625 [01:31<23:29,  2.39s/it]  6%|▌         | 37/625 [01:34<23:07,  2.36s/it]  6%|▌         | 38/625 [01:36<22:48,  2.33s/it]  6%|▌         | 39/625 [01:38<22:38,  2.32s/it]  6%|▋         | 40/625 [01:40<22:51,  2.34s/it]  7%|▋         | 41/625 [01:43<22:41,  2.33s/it]  7%|▋         | 42/625 [01:45<22:30,  2.32s/it]  7%|▋         | 43/625 [01:47<22:21,  2.31s/it]  7%|▋         | 44/625 [01:50<22:14,  2.30s/it]  7%|▋         | 45/625 [01:52<22:08,  2.29s/it]  7%|▋         | 46/625 [01:54<22:10,  2.30s/it]  8%|▊         | 47/625 [01:57<22:07,  2.30s/it]  8%|▊         | 48/625 [01:59<22:06,  2.30s/it]  8%|▊         | 49/625 [02:01<22:08,  2.31s/it]  8%|▊         | 50/625 [02:03<22:01,  2.30s/it]  8%|▊         | 51/625 [02:06<21:57,  2.30s/it]  8%|▊         | 52/625 [02:08<22:24,  2.35s/it]  8%|▊         | 53/625 [02:11<22:36,  2.37s/it]  9%|▊         | 54/625 [02:13<22:24,  2.35s/it]  9%|▉         | 55/625 [02:15<22:11,  2.34s/it]  9%|▉         | 56/625 [02:17<22:00,  2.32s/it]  9%|▉         | 57/625 [02:20<21:51,  2.31s/it]  9%|▉         | 58/625 [02:22<21:44,  2.30s/it]  9%|▉         | 59/625 [02:25<22:08,  2.35s/it] 10%|▉         | 60/625 [02:27<21:59,  2.33s/it] 10%|▉         | 61/625 [02:29<21:50,  2.32s/it] 10%|▉         | 62/625 [02:31<21:43,  2.31s/it]                                                {'loss': 1.6305, 'learning_rate': 2e-05, 'epoch': 0.5}
 10%|▉         | 62/625 [02:31<21:43,  2.31s/it] 10%|█         | 63/625 [02:34<21:36,  2.31s/it] 10%|█         | 64/625 [02:36<21:29,  2.30s/it] 10%|█         | 65/625 [02:38<21:52,  2.34s/it] 11%|█         | 66/625 [02:41<22:03,  2.37s/it] 11%|█         | 67/625 [02:43<21:47,  2.34s/it] 11%|█         | 68/625 [02:45<21:36,  2.33s/it] 11%|█         | 69/625 [02:48<21:26,  2.31s/it] 11%|█         | 70/625 [02:50<21:52,  2.36s/it] 11%|█▏        | 71/625 [02:53<21:44,  2.35s/it] 12%|█▏        | 72/625 [02:55<21:45,  2.36s/it] 12%|█▏        | 73/625 [02:57<21:29,  2.34s/it] 12%|█▏        | 74/625 [02:59<21:22,  2.33s/it] 12%|█▏        | 75/625 [03:02<21:11,  2.31s/it] 12%|█▏        | 76/625 [03:04<21:05,  2.30s/it] 12%|█▏        | 77/625 [03:06<20:59,  2.30s/it] 12%|█▏        | 78/625 [03:09<21:02,  2.31s/it] 13%|█▎        | 79/625 [03:11<21:22,  2.35s/it] 13%|█▎        | 80/625 [03:13<21:10,  2.33s/it] 13%|█▎        | 81/625 [03:16<21:02,  2.32s/it] 13%|█▎        | 82/625 [03:18<21:01,  2.32s/it] 13%|█▎        | 83/625 [03:20<20:51,  2.31s/it] 13%|█▎        | 84/625 [03:23<20:49,  2.31s/it] 14%|█▎        | 85/625 [03:25<20:52,  2.32s/it] 14%|█▍        | 86/625 [03:27<20:45,  2.31s/it] 14%|█▍        | 87/625 [03:30<20:40,  2.31s/it] 14%|█▍        | 88/625 [03:32<20:38,  2.31s/it] 14%|█▍        | 89/625 [03:34<21:00,  2.35s/it] 14%|█▍        | 90/625 [03:37<20:46,  2.33s/it] 15%|█▍        | 91/625 [03:39<20:37,  2.32s/it] 15%|█▍        | 92/625 [03:41<20:54,  2.35s/it] 15%|█▍        | 93/625 [03:44<20:42,  2.34s/it]                                                {'loss': 1.5717, 'learning_rate': 1.8898756660746003e-05, 'epoch': 0.74}
 15%|█▍        | 93/625 [03:44<20:42,  2.34s/it] 15%|█▌        | 94/625 [03:46<20:33,  2.32s/it] 15%|█▌        | 95/625 [03:48<20:54,  2.37s/it] 15%|█▌        | 96/625 [03:51<20:43,  2.35s/it] 16%|█▌        | 97/625 [03:53<20:31,  2.33s/it] 16%|█▌        | 98/625 [03:55<20:23,  2.32s/it] 16%|█▌        | 99/625 [03:58<20:40,  2.36s/it] 16%|█▌        | 100/625 [04:00<20:27,  2.34s/it] 16%|█▌        | 101/625 [04:02<20:18,  2.33s/it] 16%|█▋        | 102/625 [04:05<20:10,  2.31s/it] 16%|█▋        | 103/625 [04:07<20:04,  2.31s/it] 17%|█▋        | 104/625 [04:09<19:58,  2.30s/it] 17%|█▋        | 105/625 [04:12<20:04,  2.32s/it] 17%|█▋        | 106/625 [04:14<19:58,  2.31s/it] 17%|█▋        | 107/625 [04:16<20:14,  2.35s/it] 17%|█▋        | 108/625 [04:19<20:03,  2.33s/it] 17%|█▋        | 109/625 [04:21<19:55,  2.32s/it] 18%|█▊        | 110/625 [04:23<19:48,  2.31s/it] 18%|█▊        | 111/625 [04:25<19:43,  2.30s/it] 18%|█▊        | 112/625 [04:28<19:43,  2.31s/it] 18%|█▊        | 113/625 [04:30<19:38,  2.30s/it] 18%|█▊        | 114/625 [04:32<19:37,  2.30s/it] 18%|█▊        | 115/625 [04:35<19:45,  2.33s/it] 19%|█▊        | 116/625 [04:37<19:38,  2.32s/it] 19%|█▊        | 117/625 [04:39<19:30,  2.30s/it] 19%|█▉        | 118/625 [04:42<19:52,  2.35s/it] 19%|█▉        | 119/625 [04:44<19:42,  2.34s/it] 19%|█▉        | 120/625 [04:46<19:32,  2.32s/it] 19%|█▉        | 121/625 [04:49<19:25,  2.31s/it] 20%|█▉        | 122/625 [04:51<19:17,  2.30s/it] 20%|█▉        | 123/625 [04:53<19:17,  2.31s/it] 20%|█▉        | 124/625 [04:56<19:46,  2.37s/it]                                                 {'loss': 1.5182, 'learning_rate': 1.7797513321492008e-05, 'epoch': 0.99}
 20%|█▉        | 124/625 [04:56<19:46,  2.37s/it] 20%|██        | 125/625 [04:58<20:00,  2.40s/it] 20%|██        | 126/625 [05:00<19:43,  2.37s/it] 20%|██        | 127/625 [05:03<19:28,  2.35s/it] 20%|██        | 128/625 [05:05<19:18,  2.33s/it] 21%|██        | 129/625 [05:07<19:12,  2.32s/it] 21%|██        | 130/625 [05:10<19:30,  2.36s/it] 21%|██        | 131/625 [05:12<19:14,  2.34s/it] 21%|██        | 132/625 [05:14<19:03,  2.32s/it] 21%|██▏       | 133/625 [05:17<19:04,  2.33s/it] 21%|██▏       | 134/625 [05:19<18:56,  2.32s/it] 22%|██▏       | 135/625 [05:21<18:49,  2.31s/it] 22%|██▏       | 136/625 [05:24<18:46,  2.30s/it] 22%|██▏       | 137/625 [05:26<19:03,  2.34s/it] 22%|██▏       | 138/625 [05:28<19:01,  2.34s/it] 22%|██▏       | 139/625 [05:31<19:16,  2.38s/it] 22%|██▏       | 140/625 [05:33<19:01,  2.35s/it] 23%|██▎       | 141/625 [05:36<19:05,  2.37s/it] 23%|██▎       | 142/625 [05:39<20:34,  2.56s/it] 23%|██▎       | 143/625 [05:41<20:28,  2.55s/it] 23%|██▎       | 144/625 [05:43<20:04,  2.50s/it] 23%|██▎       | 145/625 [05:46<19:58,  2.50s/it] 23%|██▎       | 146/625 [05:48<19:50,  2.48s/it] 24%|██▎       | 147/625 [05:51<19:44,  2.48s/it] 24%|██▎       | 148/625 [05:54<20:28,  2.58s/it][2024-02-11 18:02:50,264] torch.distributed.run: [WARNING] 
[2024-02-11 18:02:50,264] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 18:02:50,264] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 18:02:50,264] torch.distributed.run: [WARNING] *****************************************
[W socket.cpp:464] [c10d] The server socket has failed to bind to [::]:18765 (errno: 98 - Address already in use).
[W socket.cpp:464] [c10d] The server socket has failed to bind to 0.0.0.0:18765 (errno: 98 - Address already in use).
[E socket.cpp:500] [c10d] The server socket has failed to listen on any local network address.
Traceback (most recent call last):
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/run.py", line 812, in main
    run(args)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    result = agent.run()
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 727, in run
    result = self._invoke_run(role)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 862, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 699, in _initialize_workers
    self._rendezvous(worker_group)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 123, in wrapper
    result = f(*args, **kwargs)
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 542, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:18765 (errno: 98 - Address already in use). The server socket has failed to bind to 0.0.0.0:18765 (errno: 98 - Address already in use).
 24%|██▍       | 149/625 [05:56<20:41,  2.61s/it] 24%|██▍       | 150/625 [05:59<19:59,  2.53s/it] 24%|██▍       | 151/625 [06:01<19:32,  2.47s/it] 24%|██▍       | 152/625 [06:03<19:13,  2.44s/it] 24%|██▍       | 153/625 [06:06<19:05,  2.43s/it] 25%|██▍       | 154/625 [06:08<18:46,  2.39s/it] 25%|██▍       | 155/625 [06:11<19:22,  2.47s/it]                                                 {'loss': 1.1636, 'learning_rate': 1.669626998223801e-05, 'epoch': 1.24}
 25%|██▍       | 155/625 [06:11<19:22,  2.47s/it] 25%|██▍       | 156/625 [06:13<18:59,  2.43s/it] 25%|██▌       | 157/625 [06:15<18:37,  2.39s/it] 25%|██▌       | 158/625 [06:18<18:46,  2.41s/it] 25%|██▌       | 159/625 [06:20<18:25,  2.37s/it] 26%|██▌       | 160/625 [06:22<18:10,  2.34s/it] 26%|██▌       | 161/625 [06:25<18:14,  2.36s/it] 26%|██▌       | 162/625 [06:27<18:01,  2.34s/it] 26%|██▌       | 163/625 [06:30<18:26,  2.39s/it][2024-02-11 18:07:28,097] torch.distributed.run: [WARNING] 
[2024-02-11 18:07:28,097] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 18:07:28,097] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-11 18:07:28,097] torch.distributed.run: [WARNING] *****************************************
[2024-02-11 18:07:39,057] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 18:07:39,057] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 18:07:39,060] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-02-11 18:07:39,063] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num_devices: 4
max_steps: 156
num_devices: 4
max_steps: 156
num_devices: 4
max_steps: 156
num_devices: 4
max_steps: 156
[2024-02-11 18:07:43,772] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 18:07:43,779] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 18:07:43,782] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 18:07:43,786] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-02-11 18:07:43,786] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[2024-02-11 18:08:00,932] [INFO] [partition_parameters.py:349:__exit__] finished initializing model - num_params = 341, num_elems = 1.42B
[2024-02-11 18:08:12,490][accelerate.utils.other][WARNING] - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-02-11 18:08:13,318] [WARNING] [engine.py:1179:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
Parameter Offload: Total persistent parameters: 544768 in 194 params
  0%|          | 0/156 [00:00<?, ?it/s]/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/mnt/vstor/CSE_CSDS_VXC204/dxg512/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/156 [00:28<1:13:01, 28.27s/it]  1%|▏         | 2/156 [00:36<42:58, 16.74s/it]    2%|▏         | 3/156 [00:45<32:50, 12.88s/it]  3%|▎         | 4/156 [00:54<29:18, 11.57s/it]  3%|▎         | 5/156 [01:04<26:59, 10.73s/it]  4%|▍         | 6/156 [01:12<24:57,  9.99s/it]  4%|▍         | 7/156 [01:21<23:41,  9.54s/it]                                               {'loss': 2.0759, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.22}
  4%|▍         | 7/156 [01:21<23:41,  9.54s/it]  5%|▌         | 8/156 [01:32<24:39, 10.00s/it]  6%|▌         | 9/156 [01:41<23:48,  9.72s/it]  6%|▋         | 10/156 [01:49<22:34,  9.28s/it]  7%|▋         | 11/156 [01:58<22:01,  9.11s/it]  8%|▊         | 12/156 [02:06<21:28,  8.95s/it]  8%|▊         | 13/156 [02:15<20:46,  8.71s/it]  9%|▉         | 14/156 [02:23<20:32,  8.68s/it]                                                {'loss': 1.7925, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.45}
  9%|▉         | 14/156 [02:23<20:32,  8.68s/it] 10%|▉         | 15/156 [02:32<20:18,  8.65s/it] 10%|█         | 16/156 [02:40<20:06,  8.62s/it] 11%|█         | 17/156 [02:49<20:21,  8.79s/it] 12%|█▏        | 18/156 [02:58<19:54,  8.66s/it] 12%|█▏        | 19/156 [03:06<19:43,  8.64s/it] 13%|█▎        | 20/156 [03:15<19:39,  8.68s/it] 13%|█▎        | 21/156 [03:25<19:59,  8.88s/it]                                                {'loss': 1.7106, 'learning_rate': 1.914893617021277e-05, 'epoch': 0.67}
 13%|█▎        | 21/156 [03:25<19:59,  8.88s/it] 14%|█▍        | 22/156 [03:33<19:32,  8.75s/it] 15%|█▍        | 23/156 [03:41<19:10,  8.65s/it] 15%|█▌        | 24/156 [03:50<18:46,  8.54s/it] 16%|█▌        | 25/156 [03:59<18:51,  8.64s/it] 17%|█▋        | 26/156 [04:07<18:34,  8.58s/it] 17%|█▋        | 27/156 [04:16<18:32,  8.62s/it] 18%|█▊        | 28/156 [04:24<18:28,  8.66s/it]                                                {'loss': 1.6164, 'learning_rate': 1.8156028368794327e-05, 'epoch': 0.9}
 18%|█▊        | 28/156 [04:24<18:28,  8.66s/it] 19%|█▊        | 29/156 [04:33<18:22,  8.68s/it] 19%|█▉        | 30/156 [04:42<18:06,  8.62s/it] 20%|█▉        | 31/156 [04:50<17:47,  8.54s/it] 21%|██        | 32/156 [04:59<17:48,  8.62s/it] 21%|██        | 33/156 [05:07<17:38,  8.60s/it] 22%|██▏       | 34/156 [05:16<17:26,  8.58s/it] 22%|██▏       | 35/156 [05:25<17:25,  8.64s/it]                                                {'loss': 1.4667, 'learning_rate': 1.716312056737589e-05, 'epoch': 1.12}
 22%|██▏       | 35/156 [05:25<17:25,  8.64s/it] 23%|██▎       | 36/156 [05:33<17:10,  8.58s/it] 24%|██▎       | 37/156 [05:42<16:54,  8.52s/it] 24%|██▍       | 38/156 [05:50<16:52,  8.58s/it] 25%|██▌       | 39/156 [06:00<17:17,  8.87s/it] 26%|██▌       | 40/156 [06:08<17:00,  8.80s/it] 26%|██▋       | 41/156 [06:17<16:36,  8.67s/it] 27%|██▋       | 42/156 [06:26<16:43,  8.80s/it]                                                {'loss': 1.3995, 'learning_rate': 1.6170212765957446e-05, 'epoch': 1.34}
 27%|██▋       | 42/156 [06:26<16:43,  8.80s/it] 28%|██▊       | 43/156 [06:34<16:21,  8.68s/it] 28%|██▊       | 44/156 [06:43<15:59,  8.57s/it] 29%|██▉       | 45/156 [06:51<15:40,  8.47s/it] 29%|██▉       | 46/156 [07:00<15:48,  8.62s/it] 30%|███       | 47/156 [07:08<15:31,  8.55s/it] 31%|███       | 48/156 [07:17<15:26,  8.58s/it] 31%|███▏      | 49/156 [07:26<15:46,  8.85s/it]                                                {'loss': 1.368, 'learning_rate': 1.5177304964539008e-05, 'epoch': 1.57}
 31%|███▏      | 49/156 [07:26<15:46,  8.85s/it] 32%|███▏      | 50/156 [07:35<15:23,  8.71s/it] 33%|███▎      | 51/156 [07:43<15:04,  8.61s/it] 33%|███▎      | 52/156 [07:52<15:01,  8.67s/it] 34%|███▍      | 53/156 [08:02<15:23,  8.96s/it] 35%|███▍      | 54/156 [08:10<14:59,  8.82s/it] 35%|███▌      | 55/156 [08:19<14:43,  8.75s/it] 36%|███▌      | 56/156 [08:27<14:39,  8.79s/it]                                                {'loss': 1.3344, 'learning_rate': 1.418439716312057e-05, 'epoch': 1.79}
 36%|███▌      | 56/156 [08:28<14:39,  8.79s/it] 37%|███▋      | 57/156 [08:36<14:24,  8.73s/it] 37%|███▋      | 58/156 [08:44<14:05,  8.63s/it] 38%|███▊      | 59/156 [08:53<14:02,  8.69s/it] 38%|███▊      | 60/156 [09:02<13:46,  8.61s/it] 39%|███▉      | 61/156 [09:10<13:35,  8.58s/it] 40%|███▉      | 62/156 [09:19<13:19,  8.51s/it] 40%|████      | 63/156 [09:28<13:25,  8.66s/it]                                                {'loss': 1.307, 'learning_rate': 1.3191489361702127e-05, 'epoch': 2.02}
 40%|████      | 63/156 [09:28<13:25,  8.66s/it] 41%|████      | 64/156 [09:36<13:19,  8.69s/it] 42%|████▏     | 65/156 [09:45<13:09,  8.67s/it] 42%|████▏     | 66/156 [09:54<13:15,  8.84s/it] 43%|████▎     | 67/156 [10:03<12:59,  8.75s/it] 44%|████▎     | 68/156 [10:11<12:41,  8.65s/it] 44%|████▍     | 69/156 [10:20<12:28,  8.60s/it] 45%|████▍     | 70/156 [10:29<12:27,  8.69s/it]                                                {'loss': 1.1444, 'learning_rate': 1.2198581560283689e-05, 'epoch': 2.24}
 45%|████▍     | 70/156 [10:29<12:27,  8.69s/it] 46%|████▌     | 71/156 [10:37<12:08,  8.57s/it] 46%|████▌     | 72/156 [10:46<12:05,  8.63s/it] 47%|████▋     | 73/156 [10:54<11:55,  8.62s/it] 47%|████▋     | 74/156 [11:03<11:47,  8.63s/it] 48%|████▊     | 75/156 [11:11<11:26,  8.47s/it] 49%|████▊     | 76/156 [11:20<11:20,  8.51s/it] 49%|████▉     | 77/156 [11:28<11:18,  8.59s/it]                                                {'loss': 1.0959, 'learning_rate': 1.120567375886525e-05, 'epoch': 2.46}
 49%|████▉     | 77/156 [11:28<11:18,  8.59s/it] 50%|█████     | 78/156 [11:37<11:07,  8.56s/it] 51%|█████     | 79/156 [11:45<10:54,  8.50s/it] 51%|█████▏    | 80/156 [11:54<10:50,  8.55s/it] 52%|█████▏    | 81/156 [12:03<10:43,  8.58s/it] 53%|█████▎    | 82/156 [12:11<10:35,  8.58s/it] 53%|█████▎    | 83/156 [12:19<10:19,  8.49s/it] 54%|█████▍    | 84/156 [12:29<10:25,  8.69s/it]                                                {'loss': 1.097, 'learning_rate': 1.0212765957446808e-05, 'epoch': 2.69}
 54%|█████▍    | 84/156 [12:29<10:25,  8.69s/it] 54%|█████▍    | 85/156 [12:37<10:19,  8.72s/it] 55%|█████▌    | 86/156 [12:46<10:02,  8.60s/it] 56%|█████▌    | 87/156 [12:54<09:52,  8.59s/it] 56%|█████▋    | 88/156 [13:03<09:38,  8.50s/it] 57%|█████▋    | 89/156 [13:11<09:24,  8.42s/it] 58%|█████▊    | 90/156 [13:19<09:15,  8.41s/it] 58%|█████▊    | 91/156 [13:29<09:32,  8.81s/it]                                                {'loss': 1.0797, 'learning_rate': 9.21985815602837e-06, 'epoch': 2.91}
 58%|█████▊    | 91/156 [13:29<09:32,  8.81s/it] 59%|█████▉    | 92/156 [13:38<09:22,  8.80s/it] 60%|█████▉    | 93/156 [13:46<09:10,  8.74s/it] 60%|██████    | 94/156 [13:55<09:03,  8.77s/it] 61%|██████    | 95/156 [14:04<08:51,  8.71s/it] 62%|██████▏   | 96/156 [14:12<08:35,  8.59s/it] 62%|██████▏   | 97/156 [14:20<08:19,  8.46s/it] 63%|██████▎   | 98/156 [14:29<08:16,  8.56s/it]                                                {'loss': 0.9781, 'learning_rate': 8.22695035460993e-06, 'epoch': 3.14}
 63%|██████▎   | 98/156 [14:29<08:16,  8.56s/it] 63%|██████▎   | 99/156 [14:37<08:06,  8.53s/it] 64%|██████▍   | 100/156 [14:46<07:53,  8.46s/it] 65%|██████▍   | 101/156 [14:54<07:50,  8.55s/it] 65%|██████▌   | 102/156 [15:03<07:43,  8.58s/it] 66%|██████▌   | 103/156 [15:12<07:32,  8.54s/it] 67%|██████▋   | 104/156 [15:20<07:26,  8.58s/it] 67%|██████▋   | 105/156 [15:29<07:22,  8.68s/it]                                                 {'loss': 0.8727, 'learning_rate': 7.234042553191491e-06, 'epoch': 3.36}
 67%|██████▋   | 105/156 [15:29<07:22,  8.68s/it] 68%|██████▊   | 106/156 [15:38<07:10,  8.61s/it] 69%|██████▊   | 107/156 [15:46<07:05,  8.69s/it] 69%|██████▉   | 108/156 [15:55<06:53,  8.62s/it] 70%|██████▉   | 109/156 [16:04<06:45,  8.62s/it] 71%|███████   | 110/156 [16:12<06:38,  8.65s/it] 71%|███████   | 111/156 [16:21<06:25,  8.57s/it] 72%|███████▏  | 112/156 [16:29<06:19,  8.63s/it]                                                 {'loss': 0.8676, 'learning_rate': 6.24113475177305e-06, 'epoch': 3.58}
 72%|███████▏  | 112/156 [16:29<06:19,  8.63s/it] 72%|███████▏  | 113/156 [16:38<06:09,  8.60s/it] 73%|███████▎  | 114/156 [16:47<06:03,  8.66s/it] 74%|███████▎  | 115/156 [16:56<05:57,  8.72s/it] 74%|███████▍  | 116/156 [17:04<05:43,  8.59s/it] 75%|███████▌  | 117/156 [17:12<05:35,  8.60s/it] 76%|███████▌  | 118/156 [17:21<05:26,  8.59s/it] 76%|███████▋  | 119/156 [17:30<05:24,  8.76s/it]                                                 {'loss': 0.8748, 'learning_rate': 5.24822695035461e-06, 'epoch': 3.81}
 76%|███████▋  | 119/156 [17:30<05:24,  8.76s/it] 77%|███████▋  | 120/156 [17:39<05:12,  8.68s/it] 78%|███████▊  | 121/156 [17:47<05:05,  8.72s/it] 78%|███████▊  | 122/156 [17:56<04:58,  8.78s/it] 79%|███████▉  | 123/156 [18:05<04:44,  8.63s/it] 79%|███████▉  | 124/156 [18:13<04:36,  8.64s/it] 80%|████████  | 125/156 [18:23<04:32,  8.79s/it] 81%|████████  | 126/156 [18:32<04:26,  8.89s/it]                                                 {'loss': 0.8538, 'learning_rate': 4.255319148936171e-06, 'epoch': 4.03}
 81%|████████  | 126/156 [18:32<04:26,  8.89s/it] 81%|████████▏ | 127/156 [18:40<04:15,  8.82s/it] 82%|████████▏ | 128/156 [18:49<04:06,  8.80s/it] 83%|████████▎ | 129/156 [18:58<04:01,  8.94s/it] 83%|████████▎ | 130/156 [19:07<03:46,  8.73s/it] 84%|████████▍ | 131/156 [19:15<03:34,  8.59s/it] 85%|████████▍ | 132/156 [19:24<03:28,  8.67s/it] 85%|████████▌ | 133/156 [19:33<03:21,  8.78s/it]                                                 {'loss': 0.7317, 'learning_rate': 3.262411347517731e-06, 'epoch': 4.26}
 85%|████████▌ | 133/156 [19:33<03:21,  8.78s/it] 86%|████████▌ | 134/156 [19:41<03:11,  8.72s/it] 87%|████████▋ | 135/156 [19:50<03:02,  8.69s/it] 87%|████████▋ | 136/156 [19:59<02:53,  8.68s/it] 88%|████████▊ | 137/156 [20:07<02:42,  8.56s/it] 88%|████████▊ | 138/156 [20:15<02:33,  8.54s/it] 89%|████████▉ | 139/156 [20:24<02:27,  8.66s/it] 90%|████████▉ | 140/156 [20:33<02:18,  8.66s/it]                                                 {'loss': 0.7087, 'learning_rate': 2.269503546099291e-06, 'epoch': 4.48}
 90%|████████▉ | 140/156 [20:33<02:18,  8.66s/it] 90%|█████████ | 141/156 [20:41<02:08,  8.58s/it] 91%|█████████ | 142/156 [20:50<01:59,  8.54s/it] 92%|█████████▏| 143/156 [20:59<01:52,  8.62s/it] 92%|█████████▏| 144/156 [21:07<01:42,  8.54s/it] 93%|█████████▎| 145/156 [21:16<01:35,  8.64s/it] 94%|█████████▎| 146/156 [21:25<01:27,  8.73s/it] 94%|█████████▍| 147/156 [21:33<01:18,  8.70s/it]                                                 {'loss': 0.717, 'learning_rate': 1.276595744680851e-06, 'epoch': 4.7}
 94%|█████████▍| 147/156 [21:33<01:18,  8.70s/it] 95%|█████████▍| 148/156 [21:42<01:09,  8.63s/it] 96%|█████████▌| 149/156 [21:50<00:59,  8.50s/it] 96%|█████████▌| 150/156 [21:59<00:51,  8.51s/it] 97%|█████████▋| 151/156 [22:07<00:42,  8.42s/it] 97%|█████████▋| 152/156 [22:16<00:34,  8.54s/it] 98%|█████████▊| 153/156 [22:24<00:25,  8.51s/it] 99%|█████████▊| 154/156 [22:32<00:16,  8.46s/it]                                                 {'loss': 0.7075, 'learning_rate': 2.8368794326241136e-07, 'epoch': 4.93}
 99%|█████████▊| 154/156 [22:32<00:16,  8.46s/it] 99%|█████████▉| 155/156 [22:41<00:08,  8.38s/it]100%|██████████| 156/156 [22:49<00:00,  8.38s/it]                                                 {'train_runtime': 1381.6259, 'train_samples_per_second': 14.453, 'train_steps_per_second': 0.113, 'train_loss': 1.166860935015556, 'epoch': 4.99}
100%|██████████| 156/156 [23:01<00:00,  8.38s/it]100%|██████████| 156/156 [23:01<00:00,  8.86s/it]
Removed shared tensor {'model.layers.22.self_attn.dense.weight', 'model.layers.12.mlp.fc2.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.7.mlp.fc1.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.7.mlp.fc2.weight', 'model.layers.16.self_attn.dense.weight', 'model.layers.9.self_attn.dense.weight', 'model.layers.1.self_attn.dense.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.8.self_attn.dense.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.5.mlp.fc2.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.3.mlp.fc1.weight', 'model.layers.2.mlp.fc1.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.5.mlp.fc1.weight', 'model.layers.22.mlp.fc1.weight', 'model.layers.15.mlp.fc2.weight', 'model.layers.22.mlp.fc2.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.0.mlp.fc1.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.19.self_attn.dense.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.1.mlp.fc2.weight', 'model.layers.18.self_attn.dense.weight', 'model.layers.21.mlp.fc2.weight', 'model.layers.20.mlp.fc2.weight', 'model.layers.17.mlp.fc2.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.mlp.fc2.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.4.mlp.fc2.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.18.mlp.fc2.weight', 'model.layers.18.mlp.fc1.weight', 'model.layers.13.mlp.fc2.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.6.mlp.fc2.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.20.self_attn.dense.weight', 'model.layers.10.mlp.fc1.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.13.self_attn.dense.weight', 'model.layers.3.mlp.fc2.weight', 'model.layers.8.mlp.fc1.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.21.self_attn.dense.weight', 'model.layers.0.mlp.fc2.weight', 'model.layers.15.mlp.fc1.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.5.self_attn.dense.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.4.self_attn.dense.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.9.mlp.fc2.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.6.mlp.fc1.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.dense.weight', 'model.layers.1.mlp.fc1.weight', 'model.layers.7.self_attn.dense.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.14.mlp.fc1.weight', 'model.layers.10.mlp.fc2.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.11.self_attn.dense.weight', 'model.layers.21.mlp.fc1.weight', 'model.layers.11.mlp.fc2.weight', 'model.layers.0.self_attn.dense.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.11.mlp.fc1.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.2.self_attn.dense.weight', 'lm_head.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.13.mlp.fc1.weight', 'model.layers.16.mlp.fc1.weight', 'model.layers.17.self_attn.dense.weight', 'model.layers.23.mlp.fc2.weight', 'model.layers.9.mlp.fc1.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.19.mlp.fc2.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.17.mlp.fc1.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.self_attn.dense.weight', 'model.layers.12.mlp.fc1.weight', 'model.layers.15.self_attn.dense.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.14.mlp.fc2.weight', 'model.layers.16.mlp.fc2.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.20.mlp.fc1.weight', 'model.layers.23.self_attn.dense.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.19.mlp.fc1.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.10.self_attn.dense.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.dense.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.8.mlp.fc2.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.23.mlp.fc1.weight', 'model.layers.6.self_attn.dense.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.4.mlp.fc1.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.20.mlp.fc2.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.3.self_attn.dense.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.0.self_attn.dense.weight', 'model.layers.8.self_attn.dense.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.18.mlp.fc1.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.17.mlp.fc1.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.self_attn.dense.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.23.self_attn.dense.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.10.self_attn.dense.weight', 'model.layers.13.self_attn.dense.weight', 'model.layers.7.mlp.fc1.weight', 'model.layers.12.self_attn.dense.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.9.mlp.fc1.weight', 'model.layers.14.self_attn.dense.weight', 'model.layers.12.mlp.fc2.weight', 'model.layers.14.mlp.fc1.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.0.mlp.fc2.weight', 'model.layers.17.self_attn.dense.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.9.self_attn.dense.weight', 'model.layers.18.self_attn.dense.weight', 'model.layers.3.mlp.fc2.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.11.mlp.fc2.weight', 'model.layers.17.mlp.fc2.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.4.mlp.fc2.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.5.mlp.fc2.weight', 'model.layers.5.mlp.fc1.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.19.mlp.fc1.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.4.self_attn.dense.weight', 'model.layers.19.mlp.fc2.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.7.mlp.fc2.weight', 'model.layers.21.mlp.fc1.weight', 'model.layers.1.self_attn.dense.weight', 'model.layers.4.mlp.fc1.weight', 'model.layers.16.mlp.fc2.weight', 'model.layers.8.mlp.fc2.weight', 'model.layers.20.self_attn.dense.weight', 'model.layers.22.self_attn.dense.weight', 'model.layers.13.mlp.fc1.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.2.mlp.fc1.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'lm_head.bias', 'model.layers.20.mlp.fc1.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.1.mlp.fc1.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.6.mlp.fc1.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.16.mlp.fc1.weight', 'model.layers.2.self_attn.dense.weight', 'model.layers.13.mlp.fc2.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.mlp.fc2.weight', 'model.layers.10.mlp.fc1.weight', 'model.layers.23.mlp.fc1.weight', 'model.layers.16.self_attn.dense.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.6.mlp.fc2.weight', 'model.layers.21.mlp.fc2.weight', 'model.layers.22.mlp.fc1.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.11.mlp.fc1.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.7.self_attn.dense.weight', 'model.layers.21.self_attn.dense.weight', 'model.layers.18.mlp.fc2.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.15.mlp.fc1.weight', 'model.layers.3.mlp.fc1.weight', 'model.layers.12.mlp.fc1.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.6.self_attn.dense.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.10.mlp.fc2.weight', 'model.layers.1.mlp.fc2.weight', 'model.layers.14.mlp.fc2.weight', 'model.layers.15.mlp.fc2.weight', 'model.layers.5.self_attn.dense.weight', 'model.layers.23.mlp.fc2.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.19.self_attn.dense.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.0.mlp.fc1.weight', 'model.layers.22.mlp.fc2.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.9.mlp.fc2.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.8.mlp.fc1.weight', 'model.layers.11.self_attn.dense.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.14.mlp.fc2.weight', 'model.layers.10.mlp.fc1.weight', 'model.layers.14.mlp.fc1.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.20.mlp.fc2.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.15.mlp.fc2.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.18.self_attn.dense.weight', 'model.layers.11.mlp.fc1.weight', 'model.layers.13.self_attn.dense.weight', 'model.layers.21.self_attn.dense.weight', 'model.layers.23.mlp.fc2.weight', 'model.layers.16.self_attn.dense.weight', 'model.layers.18.mlp.fc2.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.12.mlp.fc2.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.mlp.fc1.weight', 'model.layers.23.self_attn.dense.weight', 'model.layers.2.mlp.fc2.weight', 'model.layers.13.mlp.fc2.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.15.mlp.fc1.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.16.mlp.fc1.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.23.mlp.fc1.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.6.mlp.fc2.weight', 'model.layers.4.mlp.fc2.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.2.mlp.fc1.weight', 'model.layers.5.mlp.fc1.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.5.self_attn.dense.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.19.mlp.fc1.weight', 'model.layers.21.mlp.fc2.weight', 'model.layers.13.mlp.fc1.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.4.mlp.fc1.weight', 'model.layers.20.mlp.fc1.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.8.mlp.fc1.weight', 'model.layers.19.self_attn.dense.weight', 'model.layers.3.mlp.fc1.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.19.mlp.fc2.weight', 'model.layers.1.self_attn.dense.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.fc1.weight', 'model.layers.11.mlp.fc2.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.20.self_attn.dense.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.4.self_attn.dense.weight', 'model.layers.3.mlp.fc2.weight', 'model.layers.7.self_attn.dense.weight', 'model.layers.8.mlp.fc2.weight', 'model.layers.1.mlp.fc2.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.9.mlp.fc2.weight', 'model.layers.22.self_attn.dense.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.10.self_attn.dense.weight', 'model.layers.6.self_attn.dense.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.5.mlp.fc2.weight', 'model.layers.12.mlp.fc1.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.2.self_attn.dense.weight', 'model.layers.16.mlp.fc2.weight', 'model.layers.0.self_attn.dense.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.14.self_attn.dense.weight', 'model.layers.18.mlp.fc1.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.17.self_attn.dense.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.0.mlp.fc1.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.8.self_attn.dense.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'lm_head.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.7.mlp.fc1.weight', 'model.layers.12.self_attn.dense.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.22.mlp.fc2.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.fc2.weight', 'model.layers.3.self_attn.dense.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.dense.weight', 'model.layers.22.mlp.fc1.weight', 'model.layers.17.mlp.fc1.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.1.mlp.fc1.weight', 'model.layers.11.self_attn.dense.weight', 'model.layers.0.mlp.fc2.weight', 'model.layers.10.mlp.fc2.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.7.mlp.fc2.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.9.mlp.fc1.weight', 'model.layers.9.self_attn.dense.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.5.self_attn.v_proj.weight', 'model.layers.10.self_attn.dense.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.mlp.fc1.weight', 'model.layers.15.mlp.fc1.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.15.self_attn.dense.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.6.mlp.fc2.weight', 'model.layers.5.mlp.fc1.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.14.mlp.fc1.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.12.self_attn.dense.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.21.mlp.fc2.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.21.mlp.fc1.weight', 'model.layers.0.mlp.fc1.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.18.mlp.fc2.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.12.mlp.fc1.weight', 'model.layers.20.self_attn.dense.weight', 'model.layers.18.self_attn.q_proj.weight', 'lm_head.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.7.mlp.fc2.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.23.self_attn.dense.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.19.mlp.fc2.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.19.self_attn.dense.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.1.mlp.fc2.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.15.mlp.fc2.weight', 'model.layers.9.self_attn.dense.weight', 'model.layers.8.mlp.fc1.weight', 'model.layers.16.self_attn.dense.weight', 'model.layers.14.mlp.fc2.weight', 'model.layers.10.mlp.fc2.weight', 'model.layers.17.mlp.fc1.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.4.mlp.fc2.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.2.self_attn.dense.weight', 'model.layers.17.mlp.fc2.weight', 'model.layers.20.mlp.fc2.weight', 'model.layers.6.mlp.fc1.weight', 'model.layers.0.self_attn.dense.weight', 'model.layers.11.self_attn.dense.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.13.self_attn.dense.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.mlp.fc1.weight', 'model.layers.7.mlp.fc1.weight', 'model.layers.6.self_attn.dense.weight', 'model.layers.8.mlp.fc2.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.dense.weight', 'model.layers.16.mlp.fc2.weight', 'model.layers.4.self_attn.dense.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.4.mlp.fc1.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.2.mlp.fc2.weight', 'model.layers.18.mlp.fc1.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.7.self_attn.dense.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.9.mlp.fc2.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.mlp.fc1.weight', 'model.layers.3.mlp.fc1.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.5.mlp.fc2.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.17.self_attn.dense.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.13.mlp.fc2.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.18.self_attn.dense.weight', 'model.layers.3.mlp.fc2.weight', 'model.layers.10.mlp.fc1.weight', 'model.layers.16.mlp.fc1.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.21.self_attn.dense.weight', 'model.layers.11.mlp.fc2.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.22.self_attn.dense.weight', 'model.layers.0.mlp.fc2.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.23.mlp.fc2.weight', 'model.layers.13.mlp.fc1.weight', 'model.layers.22.mlp.fc2.weight', 'model.layers.11.mlp.fc1.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.2.mlp.fc1.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.5.self_attn.dense.weight', 'model.layers.3.self_attn.dense.weight', 'model.layers.14.self_attn.dense.weight', 'model.layers.22.mlp.fc1.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.12.mlp.fc2.weight', 'model.layers.9.mlp.fc1.weight', 'model.layers.23.mlp.fc1.weight', 'model.layers.1.self_attn.dense.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
